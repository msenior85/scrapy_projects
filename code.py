# -*- coding: utf-8 -*-
"""LeoStringMatchSCI-WORKING 12/24/2021 FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QL2jF0-cWwESYB0HPzGoXTcZ0tuyblBe

# Fast Fuzzy Matching
This notebook shows how to use TD IDF to both dedupe and match records at scale
"""

from ftfy import fix_text
import re
from matplotlib import style
import os
from tqdm import tqdm
import pandas as pd
pd.set_option('display.max_colwidth', None)
style.use('fivethirtyeight')
import numpy as np
from scipy.sparse import csr_matrix
import sparse_dot_topn.sparse_dot_topn as ct
from sklearn.feature_extraction.text import TfidfVectorizer
import time
from urllib.error import HTTPError

def read_csv(url):
    """The data can be found at the following link:
    https://drive.google.com/file/d/1EAXvkiik5EO8FcpEwfX3muQEm6cqPGrQ/view?usp=sharing
    """
    names = pd.DataFrame
    try:
        names = pd.read_csv(url, encoding="utf-8")
        msg = "Completed reading the file"
    except Exception as e:
        msg = f"Error reading source file: {e}. Exiting"
    finally:
        return msg, names


# De duplication:
def ngrams(string, n=3):
    string = str(string)
    string = fix_text(string) # fix text
    #remove non ascii chars
    string = string.encode("ascii", errors="ignore").decode()
    string = string.lower()
    chars_to_remove = [")","(",".","|","[","]","{","}","'"]
    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'
    string = re.sub(rx, '', string)
    string = string.replace('&', 'and')
    string = string.replace(',', ' ')
    string = string.replace('-', ' ')
    string = string.title() # normalise case - capital at start of each word
    # get rid of multiple spaces and replace with a single
    string = re.sub(' +',' ',string).strip()
    string = ' '+ string +' ' # pad names for ngrams...
    string = re.sub(r'[,-./]|\sBD',r'', string)
    ngrams = zip(*[string[i:] for i in range(n)])
    return [''.join(ngram) for ngram in ngrams]

def awesome_cossim_top(A, B, ntop, lower_bound=0):
    # force A and B as a CSR matrix.
    # If they have already been CSR, there is no overhead
    A = A.tocsr()
    B = B.tocsr()
    M, _ = A.shape
    _, N = B.shape

    idx_dtype = np.int32

    nnz_max = M*ntop

    indptr = np.zeros(M+1, dtype=idx_dtype)
    indices = np.zeros(nnz_max, dtype=idx_dtype)
    data = np.zeros(nnz_max, dtype=A.dtype)

    ct.sparse_dot_topn(
        M, N, np.asarray(A.indptr, dtype=idx_dtype),
        np.asarray(A.indices, dtype=idx_dtype),
        A.data,
        np.asarray(B.indptr, dtype=idx_dtype),
        np.asarray(B.indices, dtype=idx_dtype),
        B.data,
        ntop,
        lower_bound,
        indptr, indices, data)

    return csr_matrix((data,indices,indptr),shape=(M,N))


def get_tf_idf(names):
    org_names = names['Keyword'].to_numpy()
    org_index= np.array(list(names.index))
    print(org_names)
    vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)
    tf_idf_matrix = vectorizer.fit_transform(org_names)

    
    t1 = time.time()
    matches = awesome_cossim_top(
        tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85)
    t = time.time()-t1
    return t, matches, org_names, org_index

"""  # Comparison to traditional matching
This code prints the time it takes to compare < b > only one < /b > item against the population. As you can see, the TD IDF approach can match all items(3, 600) significantly faster than it takes to compare a single item using the fuzzywuzzy library.

# Inputting results into a df:
"""

def get_matches_df(sparse_matrix, name_vector,index_vector, top=100):
    non_zeros = sparse_matrix.nonzero()

    sparserows = non_zeros[0]
    sparsecols = non_zeros[1]

    if top:
        nr_matches = top
    else:
        nr_matches = sparsecols.size

    left_side = np.empty([nr_matches], dtype=object)
    right_side = np.empty([nr_matches], dtype=object)
    similairity = np.zeros(nr_matches)
    ori_index= np.empty([nr_matches], dtype=object)
    ori_index_row= np.empty([nr_matches], dtype=object)

    for index in range(0, nr_matches):
        left_side[index] = name_vector[sparsecols[index]]
        right_side[index] = name_vector[sparserows[index]]
        similairity[index] = sparse_matrix.data[index]
        ori_index[index] = index_vector[sparsecols[index]]
        ori_index_row[index] = index_vector[sparserows[index]]


    return pd.DataFrame({'Group': left_side,
                          'Keyword': right_side,
                           'similairity': similairity,
                           'ori_index_r':ori_index,
                           'ori_index':ori_index_row})

def main(names, matches, org_names, org_index):
    matches_df = get_matches_df(matches, org_names,org_index, top=False)
    #sort the similarity in descending order
    matches_df=matches_df.sort_values(['similairity'])

    #get the dataframe where the comparison is not happened on the same keyword
    temp_matches_df = matches_df[matches_df['ori_index']
        == matches_df['ori_index_r']]
    #get the dataframe where the comparison is happened on the same keyword
    matches_df = matches_df[matches_df['ori_index'] != matches_df['ori_index_r']]

    #boolean of temp_matches_df is in matches_df["Keyword"]
    temp_bool=temp_matches_df.isin(list(matches_df["Keyword"]))
    temp_matches_df=temp_matches_df[temp_bool["Keyword"] == False]

    matches_df=matches_df.sort_values(['similairity'], ).drop_duplicates(
        subset=["ori_index"], keep="first")
    #basically it just append back the removed keywords where it has no similarity > 0.85 for the comparison with other keywords
    matches_df = matches_df.append(temp_matches_df)

    matches_df=matches_df.sort_index()
    matches_df=matches_df.reset_index()

    result=pd.merge(names, matches_df, left_index=True,
                    how='left',right_on='ori_index')
    result=result.sort_values(['index'], ascending=True)
    result=result.drop(columns=['index',"ori_index_r","ori_index"])

    result.to_csv('result.csv', index=True)
    return

"""  # Record linkage
Using a similar technique to the above, we can join our messy data to a clean set of master data.
"""
